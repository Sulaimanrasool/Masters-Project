{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de583a74-8af6-4471-a4ab-4c74e177ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter,\n",
    "                                AutoMinorLocator)\n",
    "\n",
    "import warnings\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt                                       \n",
    "from scipy.integrate import solve_ivp\n",
    "from scipy.stats import norm      \n",
    "import numpy as numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003d1aea-cf1a-4a2c-9f68-c81cd56a1254",
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks = mdates.WeekdayLocator()\n",
    "years = mdates.YearLocator()\n",
    "months = mdates.MonthLocator()\n",
    "weekdays = mdates.DayLocator()\n",
    "dateFmt = mdates.DateFormatter('%b-%y')\n",
    "dateFmt_day = mdates.DateFormatter('%d-%b')\n",
    "\n",
    "\n",
    "def rolling_avg_df(dfs, N=7):\n",
    "     # Define a rolling average function\n",
    "     dfs = dfs.loc[:].rolling(N, win_type=\"boxcar\").mean()\n",
    "     return dfs\n",
    "\n",
    "query = 'https://api.coronavirus.data.gov.uk/v2/data?areaType=nation&areaName={}&metric=hospitalCases&metric=newAdmissions&metric=newCasesBySpecimenDate&metric=newDeaths28DaysByDeathDate&format=csv'\n",
    "area = 'England'\n",
    "df = pd.read_csv(query.format(area))\n",
    "df.set_index(pd.to_datetime(df['date']),inplace=True)\n",
    "df.sort_index(inplace=True)\n",
    "\n",
    "#truncate in dates if required\n",
    "df = df.truncate(before=dt.datetime(2020,7,1),after=dt.datetime(2020,12,31))\n",
    "# df.truncate(before=dt.datetime(2020,1,8),after=dt.datetime(2020,12,31))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(12,8),sharex= True, facecolor='white')\n",
    "# ax.plot(df.index,rolling_avg_df(df['newCasesBySpecimenDate']), label = 'New Cases') # daily number of new cases\n",
    "# # ax.plot(df.index,rolling_avg_df(df['newAdmissions']), label = 'new admissions') # daily number of new admissions to hospital of patients with COVID-19\n",
    "# ax.plot(df.index,rolling_avg_df(df['hospitalCases']), label = 'hospital cases') # daily number of confirmed COVID-19 patients in hospital\n",
    "# # ax.plot(df.index,rolling_avg_df(df['newDeaths28DaysByDeathDate']), label = 'deaths') # Daily numbers of people who died within 28 days of being identified as a COVID-19 case by a positive test. Data are shown by date of death.\n",
    "\n",
    "# # save data for daily number of new cases\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ax.legend()\n",
    "# ax.grid(which='both')\n",
    "# ax.set_xlabel('Date')\n",
    "# ax.set_ylabel('Counts')\n",
    "# # ax.set_yscale('log')\n",
    "# #ax.set_title('{} Cases'.format(reg))\n",
    "\n",
    "# # format the ticks\n",
    "# month_dateFmt = mdates.DateFormatter('%b')\n",
    "# ax.xaxis.set_major_locator(months)\n",
    "# ax.xaxis.set_major_formatter(dateFmt)\n",
    "# ax.xaxis.set_minor_locator(weeks)\n",
    "\n",
    "# #ax.xaxis.set_major_locator(weeks)\n",
    "# #ax.xaxis.set_major_formatter(dateFmt_day)\n",
    "# #ax.xaxis.set_minor_locator(weekdays)\n",
    "\n",
    "# ax.tick_params(axis=\"both\", direction=\"in\", which=\"both\", \n",
    "# right=True,left=True, top=True, bottom=True)\n",
    "\n",
    "# #ax.legend(ncol=1, title='')\n",
    "# _= fig.autofmt_xdate()\n",
    "\n",
    "# plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00653bb0-3a59-4aef-a9b8-777a1fd0e2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sir_odes(t, x, b, g, N):\n",
    "    \"SIR Model\"\n",
    "    \n",
    "\n",
    "    S = x[0]\n",
    "    I = x[1]\n",
    "    R = x[2]\n",
    "\n",
    "\n",
    "    dSdt = -(b/N)*S*I\n",
    "    dIdt = (b/N)*S*I - g*I\n",
    "    dRdt = g*I\n",
    "    case = (b/N)*S*I\n",
    "   \n",
    "    \n",
    "    return dSdt, dIdt, dRdt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eceef289-4b60-4337-9db7-3feab267227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cases = rolling_avg_df(df['newCasesBySpecimenDate'])\n",
    "new_cases =  new_cases.to_numpy()\n",
    "new_cases = np.array(new_cases[6:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a0b435c-2c4a-4634-874e-78d00ca38bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53999448.428571425\n"
     ]
    }
   ],
   "source": [
    "\"Set up of Model parameters\"\n",
    "lc = len(new_cases)\n",
    "t_span = np.array([1, lc])  # Time limits\n",
    "t = np.linspace(t_span[0], t_span[1], t_span[1] ) # Time series, but want to only sample for period 1: 115\n",
    "x_0 = np.array([54000000 - new_cases[0] , new_cases[0], 0])  # Initial conditions for model variables: S, I, R respectively\n",
    "N = np.sum(x_0)\n",
    "gen = 10000\n",
    "data = new_cases # Daily new cases\n",
    "print(54000000-new_cases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7862434-5dfc-4931-8f01-0ecbf87192fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sulai\\AppData\\Local\\Temp/ipykernel_19096/1245208205.py:63: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.\n",
      "  trace = pm.sample(draws = 100, chains = 1, tune = 10, cores  = 1)\n",
      "Only 100 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Initializing NUTS failed. Falling back to elementwise auto-assignment.\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Slice: [g_0]\n",
      ">Slice: [b_0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='110' class='' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [110/110 00:08<00:00 Sampling chain 0, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 1 chain for 10 tune and 100 draw iterations (10 + 100 draws total) took 8 seconds.\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "arviz - WARNING - Shape validation failed: input_shape: (1, 100), minimum_shape: (chains=2, draws=4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  r_hat\n",
      "b_0  2.521  0.001   2.520    2.523      0.001    0.001       1.0      13.0    NaN\n",
      "g_0  2.490  0.001   2.488    2.492      0.001    0.001       1.0      13.0    NaN\n"
     ]
    }
   ],
   "source": [
    "## LogLikelihood and gradient of the LogLikelihood functions\n",
    "def log_likelihood(solve_ivp, sir_odes, t_span, x_0, theta , t,  data):\n",
    "\n",
    "\n",
    "    sol = solve_ivp(sir_odes, t_span , x_0, args = theta , t_eval = t)\n",
    "    \n",
    "    S = sol.y[0]\n",
    "    I = sol.y[1]\n",
    "    \n",
    "    I = (np.multiply(S,I)*theta[0]) / theta[2]\n",
    "    # Used pandas to do truncated calcl, evolve for same number of days for the whole dfata set, including all missing variables , evolve for the same period, insert as \n",
    "    # isnert as a new column, pandas drop nans, uses pandas to truncate,insert the model values into pandas, insert I as a new column in pandas\n",
    "    # Easier to cut out of model in pandas, we tell pandas, i.e drop Nans. \n",
    "    # Dropping days that match in data and model. Then data - I would do it daya by day, we need a day by day match.  \n",
    "    logp = -0.5*np.sum(np.square((data[0:125] - I[0:125])))#/(np.std((data))**2)\n",
    "#     I = sol.y[1]\n",
    "    \n",
    "#     logp  = -0.5*np.sum(np.square((data-I)))/(np.std((data))**2)\n",
    "    return logp\n",
    "\n",
    "\n",
    "## Wrapper classes to theano-ize LogLklhood and gradient...\n",
    "class Loglike(tt.Op):\n",
    "    itypes = [tt.dvector]\n",
    "    otypes = [tt.dscalar]\n",
    "\n",
    "    def __init__(self, solve_ivp, sir_odes, t_span,x_0, t, data):\n",
    "        self.data = data\n",
    "        self.solve_ivp = solve_ivp\n",
    "        self.sir_odes = sir_odes\n",
    "        self.t_span = t_span\n",
    "        self.x_0 = x_0\n",
    "        self.t = t\n",
    "        # self.loglike_grad = LoglikeGrad(self.data, self.t)\n",
    "\n",
    "    def perform(self, node, inputs, outputs):\n",
    "        (theta,) = inputs \n",
    "        logp = log_likelihood(self.solve_ivp,self.sir_odes, self.t_span,self.x_0, theta , self.t, self.data)\n",
    "        \n",
    "        outputs[0][0] = np.array(logp)\n",
    "\n",
    "\n",
    "chi_store = []\n",
    "loglike = Loglike(solve_ivp, sir_odes, t_span,x_0, t, data)\n",
    "with pm.Model() as model:\n",
    "    sigma = 0.5 #pm.HalfNormal('sigma' ,sd = 10)\n",
    "\n",
    "    # b_0 = pm.Normal('b_0', mu = 5, sigma= 10)\n",
    "    b_0 = pm.Uniform('b_0',lower = 0, upper = 5)\n",
    "     # split beta into five seperate paramers, then add an if statment into the integrator, then say if t = 5 then use this beta\n",
    "    # g_0 = pm.Normal('g_0', mu = 5 , sigma= 10)\n",
    "    g_0 = pm.Uniform('g_0', lower = 0, upper = 5)\n",
    "    \n",
    "    \n",
    "    theta = tt.as_tensor_variable([b_0,g_0,N])\n",
    "    \n",
    "    pm.Potential(\"like\", loglike(theta))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "with model:\n",
    "    trace = pm.sample(draws = 100, chains = 1, tune = 10, cores  = 1)\n",
    "    # trace = pm.sample(step=pm.NUTS())\n",
    "    # idata = pm.sample(return_inferencedata=True)\n",
    "    print(pm.summary(trace).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78a7b6ba-99b5-4974-b114-d4303510aaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         2.52308597 2.49199622]\n",
      " [1.         2.52307581 2.49193128]\n",
      " [1.         2.52305565 2.49192961]\n",
      " [1.         2.52300386 2.49190314]\n",
      " [1.         2.52298834 2.49185334]\n",
      " [1.         2.52297462 2.49185085]\n",
      " [1.         2.5229682  2.49181246]\n",
      " [1.         2.5229297  2.49182918]\n",
      " [1.         2.52283548 2.4917693 ]\n",
      " [1.         2.52276375 2.49164296]\n",
      " [1.         2.52273248 2.49159935]\n",
      " [1.         2.52268981 2.49158825]\n",
      " [1.         2.52269301 2.49158664]\n",
      " [1.         2.52264305 2.49152156]\n",
      " [1.         2.52261338 2.49149786]\n",
      " [1.         2.52258375 2.49147213]\n",
      " [1.         2.52252598 2.49144848]\n",
      " [1.         2.52240156 2.49133649]\n",
      " [1.         2.52239298 2.49122407]\n",
      " [1.         2.52237468 2.49126204]\n",
      " [1.         2.5223428  2.49122968]\n",
      " [1.         2.52231791 2.49117149]\n",
      " [1.         2.52228536 2.49117105]\n",
      " [1.         2.52228396 2.49116809]\n",
      " [1.         2.52227549 2.49115542]\n",
      " [1.         2.52226065 2.49112926]\n",
      " [1.         2.52225203 2.49112526]\n",
      " [1.         2.52218799 2.49110879]\n",
      " [1.         2.52205741 2.4909877 ]\n",
      " [1.         2.52209196 2.49097412]\n",
      " [1.         2.52206694 2.49093472]\n",
      " [1.         2.52204038 2.49091693]\n",
      " [1.         2.52203648 2.49091661]\n",
      " [1.         2.5220248  2.49087081]\n",
      " [1.         2.52196151 2.49087246]\n",
      " [1.         2.52196333 2.49086643]\n",
      " [1.         2.52194429 2.49081104]\n",
      " [1.         2.52190807 2.4907962 ]\n",
      " [1.         2.52190705 2.49074811]\n",
      " [1.         2.52184664 2.49074976]\n",
      " [1.         2.52179484 2.49066772]\n",
      " [1.         2.52174175 2.49065742]\n",
      " [1.         2.52163015 2.49056273]\n",
      " [1.         2.52160749 2.49050461]\n",
      " [1.         2.52160597 2.49049991]\n",
      " [1.         2.52152425 2.49041995]\n",
      " [1.         2.52151573 2.4904126 ]\n",
      " [1.         2.52147749 2.49038642]\n",
      " [1.         2.52143581 2.49029922]\n",
      " [1.         2.52139008 2.49029588]\n",
      " [1.         2.52136678 2.49021908]\n",
      " [1.         2.52136412 2.49021906]\n",
      " [1.         2.52134789 2.49021653]\n",
      " [1.         2.52127911 2.49019509]\n",
      " [1.         2.52122534 2.49007693]\n",
      " [1.         2.5211667  2.49007803]\n",
      " [1.         2.52115571 2.49005292]\n",
      " [1.         2.52114729 2.49003914]\n",
      " [1.         2.52112853 2.49001517]\n",
      " [1.         2.52108658 2.48994843]\n",
      " [1.         2.52102253 2.48994174]\n",
      " [1.         2.521036   2.48992226]\n",
      " [1.         2.5209616  2.48985623]\n",
      " [1.         2.52090215 2.48978052]\n",
      " [1.         2.52087899 2.48977732]\n",
      " [1.         2.52082161 2.48970405]\n",
      " [1.         2.52077672 2.48967088]\n",
      " [1.         2.52077419 2.48965746]\n",
      " [1.         2.52074345 2.48963496]\n",
      " [1.         2.52067291 2.48957229]\n",
      " [1.         2.52064113 2.48954084]\n",
      " [1.         2.52061969 2.48950449]\n",
      " [1.         2.52052465 2.48945843]\n",
      " [1.         2.52035201 2.48930112]\n",
      " [1.         2.52035173 2.48924215]\n",
      " [1.         2.52033593 2.48923084]\n",
      " [1.         2.52030935 2.48918707]\n",
      " [1.         2.52025649 2.4891679 ]\n",
      " [1.         2.52025734 2.48915042]\n",
      " [1.         2.52024112 2.48910676]\n",
      " [1.         2.52018509 2.48908285]\n",
      " [1.         2.52017039 2.4890477 ]\n",
      " [1.         2.52013179 2.48902898]\n",
      " [1.         2.52012729 2.48899591]\n",
      " [1.         2.52006131 2.48896249]\n",
      " [1.         2.51995242 2.4888625 ]\n",
      " [1.         2.51994672 2.48875529]\n",
      " [1.         2.51988568 2.48876765]\n",
      " [1.         2.51986783 2.48874967]\n",
      " [1.         2.51985867 2.48873463]\n",
      " [1.         2.519788   2.4887142 ]\n",
      " [1.         2.51974744 2.48858485]\n",
      " [1.         2.51969317 2.48861026]\n",
      " [1.         2.51961329 2.48850379]\n",
      " [1.         2.51961134 2.48849743]\n",
      " [1.         2.51959853 2.48848493]\n",
      " [1.         2.51955192 2.48845798]\n",
      " [1.         2.5195094  2.48838856]\n",
      " [1.         2.51946622 2.48836352]\n",
      " [1.         2.51941561 2.48831959]]\n"
     ]
    }
   ],
   "source": [
    "b_0 = trace[\"b_0\"]\n",
    "g_0 = trace[\"g_0\"]\n",
    "\n",
    "chi_store = []\n",
    "for b_0, g_0 in zip(b_0, g_0 ):\n",
    "    \n",
    "#     args = (b_a, b_sy, g_a, g_sy, e_a, e_sy, a, N) \n",
    "#     sol = solve_ivp(seasyr_odes, t_span, x_0, args=(args), t_eval=t)\n",
    "    \n",
    "#     E = sol.y[1]\n",
    "    \n",
    "    chi_value = 1\n",
    "    chi_store.append(chi_value)\n",
    "    \n",
    "chains = np.column_stack([ chi_store, trace[\"b_0\"] ,trace[\"g_0\"]])\n",
    "print(chains)\n",
    "datafile_path = \"chain_SIR_con\"\n",
    "np.savetxt(datafile_path , chains, fmt=['%f','%f','%f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd01bcd-dfe6-4338-86cb-ffc297b11f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_span = np.array([1, lc])  # Time limits\n",
    "t = np.linspace(t_span[0], t_span[1], t_span[1] ) # Time series, but want to only sample for period 1: 115\n",
    "x_0 = np.array([54000000 - new_cases[0] , new_cases[0], 0])  # Initial conditions for model variables: S, I, R respectively\n",
    "N = np.sum(x_0)\n",
    "# args = [2.386, 2.355, N]\n",
    "args = [2.394, 2.362, N]\n",
    "sol = solve_ivp(sir_odes, t_span, x_0, args=args, t_eval=t)\n",
    "S = sol.y[0]\n",
    "I = sol.y[1]\n",
    "I = (np.multiply(S,I)*args[0]) / args[2]\n",
    "# plt.plot(t, I, label=\"Model\")\n",
    "# plt.plot(t, data, label=\"Data\")\n",
    "# plt.legend()\n",
    "# # truncate to day 60 to catagorise the peak. \n",
    "# # Add an r that is binwise overa certain periods, this is r_t\n",
    "# # rough thing to quantify,, calcaulte loglike / number of days, look for goodness of fit statistic, want to comapre this to antoher model with a different parameter set\n",
    "# plt.show()\n",
    "dates1 = pd.date_range(start='07/07/2020', end='31/12/2020', periods = 178)\n",
    "plt.figure(figsize=(15,9))\n",
    "# plt.plot(dates1, S)\n",
    "plt.plot(dates1,data,label = 'Daily new infection cases',color='teal')\n",
    "plt.plot(dates1[0:126],I[0:126],label = r'Model daily new  infection cases', color = 'r')\n",
    "plt.plot(dates1[125:],I[125:],label = r'Predicted daily new  infection cases', color = 'b', ls = '--')\n",
    "plt.xlabel(\"Date\", fontsize = 17)\n",
    "plt.ylabel(\"Population\", fontsize = 17)\n",
    "plt.axvline(dates1[125:126], c = 'm',ls = '--', label = 'Cut-Off')\n",
    "plt.legend(fontsize=13)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.grid()\n",
    "plt.savefig('SIR,,pred')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plt.plot(t,I_data/N,label = 'Infected Data',color='teal')\n",
    "# plt.plot(t,I/N,label = 'Infected Signal',color='black')\n",
    "# plt.plot(t,H_data/N,label = 'Hospitalised Data',color='green')\n",
    "# plt.plot(t,H/N,label = 'Hospitalised Signal',color='m')\n",
    "\n",
    "# plt.xlabel(\"Time[Days]\", fontsize = 15)\n",
    "# plt.ylabel(\"Fraction of Population\", fontsize = 15)\n",
    "\n",
    "\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f687f5d7-d5b0-4023-856c-e5f55a1c01ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d9785d-4002-403a-aacd-3297f686affc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
